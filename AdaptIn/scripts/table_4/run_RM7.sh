python ../../Seq_Rec_Model/AdaptIn_wo_LN.py  --mode="train-test" \
				--use-gpu \
				--arch-interaction-op=transformers \
				--num-encoder-layers=1 \
				--num-attention-heads=2 \
				--feedforward-dim=128 \
				--dropout=0.01 \
				--norm-first=True \
				--activation=relu \
				--dlrm-path=../../Seq_Rec_Model/dlrm \
				--datatype="taobao" \
				--model-type="transformer" \
				--tra-encoder-layers=1 \
				--tra-attention-heads=2 \
				--tra-feedforward-dim=512 \
				--tra-norm-first=True \
				--tra-activation=relu \
				--num-train-pts=690000 \
				--num-val-pts=300000 \
				--points-per-user=10 \
				--mini-batch-size=128 \
				--nepochs=1 \
				--numpy-rand-seed=123 \
				--arch-embedding-size="987994-4162024-9439" \
				--print-freq=4096 \
				--test-freq=4096 \
				--num-batches=0 \
				--raw-train-file=<path_to_train.text> \
				--raw-test-file=<path_to_test.txt> \
				--pro-train-file=<path_to_taobao_train.npz> \
				--pro-val-file=<[ath_to_taobao_val.npz> \
				--pro-test-file=<path_to_taobao_test.npz> \
				--save-model=./output/model.pt \
				--ts-length=20 \
				--device-num=0 \
				--tsl-interaction-op="dot" \
				--tsl-mechanism="mlp" \
				--learning-rate=0.05 \
				--arch-sparse-feature-size=16 \
				--arch-mlp-bot="1-16" \
				--arch-mlp-top="15-16" \
				--tsl-mlp="15-15" \
				--arch-mlp="60-1" \
				--mask-threshold=0.001-0.01
